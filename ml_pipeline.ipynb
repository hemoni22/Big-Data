{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02803b31-a95a-4511-8cd0-ef1575deb2ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = spark.table(\"workspace.default.credit_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bae2c384-2da9-47e8-81cc-92b0f8f7c4aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     amount  is_international  ...  label_fraud  label_fraud_encoded\n0 -0.080614         -0.294371  ...        False                    0\n1  5.410233         -0.294371  ...        False                    0\n2 -0.538184          3.397076  ...        False                    0\n3 -0.538184          3.397076  ...        False                    0\n4  0.376957         -0.294371  ...        False                    0\n\n[5 rows x 7 columns]\nTraining samples: 400000, Test samples: 100000\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# \uD83D\uDD39 Step 1: Import Libraries\n",
    "# ==========================================================\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemb le import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# ==========================================================\n",
    "# \uD83D\uDD39 Step 2: Load Spark Table\n",
    "# ==========================================================\n",
    "df = spark.table(\"workspace.default.credit_score\")\n",
    "\n",
    "# ==========================================================\n",
    "# \uD83D\uDD39 Step 3: Compute Credit Score\n",
    "# ==========================================================\n",
    "df_score = df.withColumn(\n",
    "    \"credit_score\",\n",
    "    (\n",
    "        850\n",
    "        - (F.col(\"amount\") * 10)\n",
    "        - (F.when(F.col(\"is_international\") == True, 50).otherwise(0))\n",
    "        - (F.when(F.col(\"label_fraud\") == True, 150).otherwise(0))\n",
    "        + (F.when(F.col(\"is_chip\") == True, 20).otherwise(0))\n",
    "        + (F.when(F.col(\"is_contactless\") == True, 10).otherwise(0))\n",
    "    ).cast(\"int\")\n",
    ")\n",
    "\n",
    "# Cap between 400 and 850\n",
    "df_score = df_score.withColumn(\n",
    "    \"credit_score\",\n",
    "    F.when(F.col(\"credit_score\") > 850, 850)\n",
    "     .when(F.col(\"credit_score\") < 400, 400)\n",
    "     .otherwise(F.col(\"credit_score\"))\n",
    ")\n",
    "\n",
    "# Convert to Pandas\n",
    "pdf = df_score.toPandas()\n",
    "\n",
    "# ==========================================================\n",
    "# \uD83D\uDD39 Step 4: Preprocessing\n",
    "# ==========================================================\n",
    "# 1️⃣ Select features and target\n",
    "features = [\"amount\", \"is_international\", \"is_chip\", \"is_contactless\", \"credit_score\"]\n",
    "target = \"label_fraud\"\n",
    "\n",
    "# 2️⃣ Handle missing values (if any)\n",
    "pdf[features] = pdf[features].fillna(0)  # fill numeric/boolean features with 0\n",
    "pdf[target] = pdf[target].fillna(False)  # fill target boolean with False\n",
    "\n",
    "# 3️⃣ Convert boolean columns to integer (0/1)\n",
    "pdf[features] = pdf[features].astype(int)\n",
    "\n",
    "# 4️⃣ Encode target label\n",
    "le = LabelEncoder()\n",
    "pdf[target + \"_encoded\"] = le.fit_transform(pdf[target])\n",
    "\n",
    "# 5️⃣ Optional: Feature scaling (for some models, not strictly needed for Random Forest)\n",
    "scaler = StandardScaler()\n",
    "pdf[features] = scaler.fit_transform(pdf[features])\n",
    "\n",
    "# Check preprocessed data\n",
    "print(pdf[features + [target, target+\"_encoded\"]].head())\n",
    "\n",
    "# ==========================================================\n",
    "# \uD83D\uDD39 Step 5: Split Dataset\n",
    "# ==========================================================\n",
    "X = pdf[features]\n",
    "y = pdf[target + \"_encoded\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}, Test samples: {X_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91966810-12d5-4fc6-af15-7cdd39578f12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Accuracy: 100.00%\nConfusion Matrix:\n[[99028     0]\n [    0   972]]\nClassification Report:\n              precision    recall  f1-score   support\n\n   Non-Fraud       1.00      1.00      1.00     99028\n       Fraud       1.00      1.00      1.00       972\n\n    accuracy                           1.00    100000\n   macro avg       1.00      1.00      1.00    100000\nweighted avg       1.00      1.00      1.00    100000\n\nCredit Score: 570\nPredicted Fraudulent Transaction: False\nFraud Probability: 1.00%\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# \uD83D\uDD39 Step 1: Import Libraries\n",
    "# ==========================================================\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# ==========================================================\n",
    "# \uD83D\uDD39 Step 2: Load Spark Table\n",
    "# ==========================================================\n",
    "df = spark.table(\"workspace.default.credit_score\")\n",
    "\n",
    "# ==========================================================\n",
    "# \uD83D\uDD39 Step 3: Compute credit_score dynamically\n",
    "# ==========================================================\n",
    "df_score = df.withColumn(\n",
    "    \"credit_score\",\n",
    "    (\n",
    "        850\n",
    "        - (F.col(\"amount\") * 10)\n",
    "        - (F.when(F.col(\"is_international\") == True, 50).otherwise(0))\n",
    "        - (F.when(F.col(\"label_fraud\") == True, 150).otherwise(0))\n",
    "        + (F.when(F.col(\"is_chip\") == True, 20).otherwise(0))\n",
    "        + (F.when(F.col(\"is_contactless\") == True, 10).otherwise(0))\n",
    "    ).cast(\"int\")\n",
    ")\n",
    "\n",
    "# Cap between 400 and 850\n",
    "df_score = df_score.withColumn(\n",
    "    \"credit_score\",\n",
    "    F.when(F.col(\"credit_score\") > 850, 850)\n",
    "     .when(F.col(\"credit_score\") < 400, 400)\n",
    "     .otherwise(F.col(\"credit_score\"))\n",
    ")\n",
    "\n",
    "# Convert to Pandas\n",
    "pdf = df_score.toPandas()\n",
    "\n",
    "# ==========================================================\n",
    "# \uD83D\uDD39 Step 4: Preprocessing\n",
    "# ==========================================================\n",
    "# Features for fraud prediction\n",
    "features = [\"amount\", \"is_international\", \"is_chip\", \"is_contactless\", \"credit_score\"]\n",
    "\n",
    "# Ensure boolean columns are integers\n",
    "pdf[features] = pdf[features].astype(int)\n",
    "\n",
    "# Target variable\n",
    "target = \"label_fraud\"\n",
    "le = LabelEncoder()\n",
    "pdf[target + \"_encoded\"] = le.fit_transform(pdf[target])\n",
    "\n",
    "# ==========================================================\n",
    "# \uD83D\uDD39 Step 5: Split Dataset\n",
    "# ==========================================================\n",
    "X = pdf[features]\n",
    "y = pdf[target + \"_encoded\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ==========================================================\n",
    "# \uD83D\uDD39 Step 6: Model Training\n",
    "# ==========================================================\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# ==========================================================\n",
    "# \uD83D\uDD39 Step 7: Model Evaluation\n",
    "# ==========================================================\n",
    "y_pred = rf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=[\"Non-Fraud\",\"Fraud\"])\n",
    "\n",
    "print(f\"✅ Accuracy: {accuracy*100:.2f}%\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# ==========================================================\n",
    "# \uD83D\uDD39 Step 8: Prediction Function\n",
    "# ==========================================================\n",
    "def predict_fraud(amount, is_international, is_chip, is_contactless):\n",
    "    credit_score = 850 \\\n",
    "        - (amount * 10) \\\n",
    "        - (50 if is_international else 0) \\\n",
    "        + (20 if is_chip else 0) \\\n",
    "        + (10 if is_contactless else 0)\n",
    "    credit_score = max(400, min(850, credit_score))\n",
    "    \n",
    "    input_df = pd.DataFrame({\n",
    "        \"amount\": [amount],\n",
    "        \"is_international\": [int(is_international)],\n",
    "        \"is_chip\": [int(is_chip)],\n",
    "        \"is_contactless\": [int(is_contactless)],\n",
    "        \"credit_score\": [credit_score]\n",
    "    })\n",
    "    \n",
    "    pred_encoded = rf.predict(input_df)[0]\n",
    "    pred_label = le.inverse_transform([pred_encoded])[0]\n",
    "    pred_prob = rf.predict_proba(input_df)[0][1]\n",
    "    \n",
    "    return pred_label, pred_prob, credit_score\n",
    "\n",
    "# ==========================================================\n",
    "# \uD83D\uDD39 Step 9: Test Prediction\n",
    "# ==========================================================\n",
    "amount = 25\n",
    "is_international = True\n",
    "is_chip = True\n",
    "is_contactless = False\n",
    "\n",
    "pred_label, pred_prob, credit_score = predict_fraud(amount, is_international, is_chip, is_contactless)\n",
    "print(f\"Credit Score: {credit_score}\")\n",
    "print(f\"Predicted Fraudulent Transaction: {pred_label}\")\n",
    "print(f\"Fraud Probability: {pred_prob*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "252eb5d4-2171-4d49-bdf9-a5835d8ffac7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ml_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}